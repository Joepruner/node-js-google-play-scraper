{
  "_from": "csv-parser@2.2.0",
  "_id": "csv-parser@2.2.0",
  "_inBundle": false,
  "_integrity": "sha512-d2USCnRP5fSMHEPfVdzIxlk7+zpFlffak5Ad4Ndzm3Nw1rmxBxezpyL4A3FD1g4R+W/cx2TEeKnGdTz49BJyRg==",
  "_location": "/csv-parser",
  "_phantomChildren": {},
  "_requested": {
    "escapedName": "csv-parser",
    "fetchSpec": "2.2.0",
    "name": "csv-parser",
    "raw": "csv-parser@2.2.0",
    "rawSpec": "2.2.0",
    "registry": true,
    "saveSpec": null,
    "type": "version"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/csv-parser/-/csv-parser-2.2.0.tgz",
  "_shasum": "241dc4d02854f1b05965895219988338240d833a",
  "_shrinkwrap": null,
  "_spec": "csv-parser@2.2.0",
  "_where": "/home/joepruner/Projects/GooglePlayScraper/public/javascripts",
  "author": {
    "name": "mafintosh"
  },
  "bin": {
    "csv-parser": "./bin/csv-parser"
  },
  "bugs": {
    "url": "https://github.com/mafintosh/csv-parser/issues"
  },
  "bundleDependencies": false,
  "dependencies": {
    "buffer-alloc": "^1.1.0",
    "buffer-from": "^1.0.0",
    "execa": "^1.0.0",
    "generate-function": "^1.0.1",
    "generate-object-property": "^1.0.0",
    "minimist": "^1.2.0",
    "ndjson": "^1.4.0"
  },
  "deprecated": false,
  "description": "Streaming CSV parser that aims for maximum speed as well as compatibility with the csv-spectrum test suite",
  "devDependencies": {
    "@commitlint/cli": "^7.0.0",
    "@commitlint/config-conventional": "^7.0.1",
    "ava": "^0.25.0",
    "bops": "^0.1.1",
    "chalk": "^2.4.1",
    "concat-stream": "^1.4.5",
    "csv-spectrum": "^1.0.0",
    "eslint": "^5.4.0",
    "eslint-config-standard": "^11.0.0",
    "eslint-plugin-import": "^2.14.0",
    "eslint-plugin-node": "^7.0.1",
    "eslint-plugin-promise": "^4.0.0",
    "eslint-plugin-standard": "^3.1.0",
    "globby": "^8.0.1",
    "husky": "^0.14.3",
    "lint-staged": "^7.2.0",
    "loud-rejection": "^1.6.0",
    "pre-commit": "^1.2.2",
    "standard-version": "^4.4.0",
    "strip-ansi": "^4.0.0",
    "text-table": "^0.2.0",
    "time-span": "^2.0.0"
  },
  "directories": {
    "example": "examples",
    "test": "test"
  },
  "engines": {
    "node": ">= 6.14.0"
  },
  "files": [
    "bin/csv-parser",
    "index.d.ts",
    "index.js"
  ],
  "homepage": "https://github.com/mafintosh/csv-parser",
  "keywords": [
    "csv",
    "fast",
    "json",
    "parser"
  ],
  "license": "MIT",
  "lint-staged": {
    "*.js": [
      "eslint --fix",
      "git add"
    ]
  },
  "main": "index.js",
  "maintainers": [
    {
      "name": "Andrew Powell",
      "email": "andrew@shellscape.org"
    }
  ],
  "name": "csv-parser",
  "optionalDependencies": {},
  "pre-commit": "lint-staged",
  "readme": "[tests]: \thttp://img.shields.io/travis/mafintosh/csv-parser.svg\n[tests-url]: http://travis-ci.org/mafintosh/csv-parser\n\n[cover]: https://codecov.io/gh/mafintosh/csv-parser/branch/master/graph/badge.svg\n[cover-url]: https://codecov.io/gh/mafintosh/csv-parser\n\n[size]: https://packagephobia.now.sh/badge?p=csv-parser\n[size-url]: https://packagephobia.now.sh/result?p=csv-parser\n\n# csv-parser\n\n[![tests][tests]][tests-url]\n[![cover][cover]][cover-url]\n[![size][size]][size-url]\n\nStreaming CSV parser that aims for maximum speed as well as compatibility with\nthe [csv-spectrum](https://npmjs.org/csv-spectrum) CSV acid test suite.\n\n`csv-parser` can convert CSV into JSON at at rate of around 90,000 rows per\nsecond. Performance varies with the data used; try `bin/bench.js <your file>`\nto benchmark your data.\n\n`csv-parser` can be used in the browser with [browserify](http://browserify.org/).\n\n[neat-csv](https://github.com/sindresorhus/neat-csv) can be used if a `Promise`\nbased interface to `csv-parser` is needed.\n\n_Note: This module requires Node v6.14.0 or higher._\n\n## Benchmarks\n\n⚡️ `csv-parser` is greased-lightning fast\n\n```console\n→ npm run bench\n\n\n  Filename                                   Rows Parsed  Duration\n  comma_in_quotes.csv                                  1     4.8ms\n  custom_escape_character.csv                          3    0.69ms\n  custom_quote_and_escape_character.csv                3    0.85ms\n  custom_quote_character.csv                           2    0.71ms\n  custom_quote_character_default_escape.csv            3    0.78ms\n  dummy.csv                                            1    0.75ms\n  escaped_quotes.csv                                   3    0.77ms\n  empty_columns.csv                                    1    0.83ms\n  junk_rows.csv                                        3    0.83ms\n  mac_newlines.csv                                     2    0.67ms\n  newlines.csv                                         3    0.61ms\n  process_all_rows.csv                              7268      78ms\n  quotes_and_newlines.csv                              3     1.1ms\n  test_geojson.csv                                     3     2.6ms\n  test_latin1.csv                                      2    0.76ms\n  test_strict.csv                                      3    0.70ms\n  test_utf16_big.csv                                   2     1.0ms\n  test_utf16_little.csv                                2    0.59ms\n  test_utf8.csv                                        2    0.59ms\n```\n\n## Install\n\nUsing npm:\n\n```console\n$ npm install csv-parser\n```\n\nUsing yarn:\n\n```console\n$ yarn add csv-parser\n```\n\n## Usage\n\nTo use the module, create a readable stream to a desired CSV file, instantiate\n`csv`, and pipe the stream to `csv`.\n\nSuppose you have a CSV file `data.csv` which contains the data:\n\n```\nNAME,AGE\nDaffy Duck,24\nBugs Bunny,22\n```\n\nIt could then be parsed, and results shown like so:\n\n``` js\nconst csv = require('csv-parser')\nconst fs = require('fs')\nconst results = [];\n\nfs.createReadStream('data.csv')\n  .pipe(csv())\n  .on('data', (data) => results.push(data))\n  .on('end', () => {\n    console.log(results);\n    // [\n    //   { NAME: 'Daffy Duck', AGE: '24' },\n    //   { NAME: 'Bugs Bunny', AGE: '22' }\n    // ]\n  });\n```\n\nTo specify options for `csv`, pass an object argument to the function. For\nexample:\n\n```js\ncsv({ separator: '\\t' });\n```\n\n## API\n\n### csv([options|headers])\n\nReturns: `Array[object]`\n\n#### options\n\nType: `object`\n\nAs an alternative to passing an `options` object, you may pass an `Array[String]`\nwhich specifies the headers to use. For example:\n\n```js\ncsv(['Name', 'Age']);\n```\n\nIf you need to specify options _and_ headers, please use the the object notation\nwith the `headers` property as shown below.\n\n##### escape\n\nType: `String`<br>\nDefault: `\"`\n\nA single-character string used to specify the character used to escape strings\nin a CSV row.\n\n##### headers\n\nType: `Array[String]|boolean`\n\nSpecifies the headers to use. Headers define the property key for each value in\na CSV row. If no `headers` option is provided, `csv-parser` will use the first\nline in a CSV file as the header specification.\n\nIf `false`, specifies that the first row in a data file does _not_ contain\nheaders, and instructs the parser to use the row index as the key for each row.\nUsing `headers: false` with the same `data.csv` example from above would yield:\n\n``` js\n[\n  { '0': 'Daffy Duck', '1': 24 },\n  { '0': 'Bugs Bunny', '1': 22 }\n]\n```\n\n##### mapHeaders\n\nType: `Function`\n\nA function that can be used to modify the values of each header. Return `null`\nto remove the header, and it's column, from the results.\n\n```js\ncsv({\n  mapHeaders: ({ header, index }) => header.toLowerCase()\n})\n```\n\n##### mapValues\n\nType: `Function`\n\nA function that can be used to modify the value of each column value.\n\n```js\ncsv({\n  mapValues: ({ header, index, value }) => value.toLowerCase()\n})\n```\n\n##### newline\n\nType: `String`<br>\nDefault: `\\n`\n\nSpecifies a single-character string to denote the end of a line in a CSV file.\n\n##### quote\n\nType: `String`<br>\nDefault: `\"`\n\nSpecifies a single-character string to denote a quoted string.\n\n##### raw\n\nType: `Boolean`<br>\n\nIf `true`, instructs the parser not to decode UTF-8 strings.\n\n##### separator\n\nType: `String`<br>\nDefault: `,`\n\nSpecifies a single-character string to use as the column separator for each row.\n\n##### skipComments\n\nType: `Boolean | String`<br>\nDefault: `false`\n\nInstructs the parser to ignore lines which represent comments in a CSV file. Since there is no specification that dictates what a CSV comment looks like, comments should be considered non-standard. The \"most common\" character used to signify a comment in a CSV file is `\"#\"`. If this option is set to `true`, lines which begin with `#` will be skipped. If a custom character is needed to denote a commented line, this option may be set to a string which represents the leading character(s) signifying a comment line.\n\n##### skipLines\n\nType: `Number`<br>\nDefault: `0`\n\nSpecifies the number of lines at the beginning of a data file that the parser should\nskip over, prior to parsing headers.\n\n##### maxRowBytes\n\nType: `Number`<br>\nDefault: `Number.MAX_SAFE_INTEGER`\n\nMaximum number of bytes per row. An error is thrown if a line exeeds this value. The default value is on 8 peta byte.\n\n##### strict\n\nType: `Boolean`<br>\n\nIf `true`, instructs the parser that the number of columns in each row must match\nthe number of `headers` specified.\n\n## Events\n\nThe following events are emitted during parsing:\n\n### `data`\n\nEmitted for each row of data parsed with the notable exception of the header\nrow. Please see [Usage](#Usage) for an example.\n\n### `headers`\n\nEmitted after the header row is parsed. The first parameter of the event\ncallback is an `Array[String]` containing the header names.\n\n```js\nfs.createReadStream('data.csv')\n  .pipe(csv())\n  .on('headers', (headers) => {\n    console.log(`First header: ${headers[0]}`)\n  })\n```\n\n### Readable Stream Events\n\nEvents available on Node built-in\n[Readable Streams](https://nodejs.org/api/stream.html#stream_class_stream_readable)\nare also emitted. The `end` event should be used to detect the end of parsing.\n\n## CLI\n\nThis module also provides a CLI which will convert CSV to\n[newline-delimited](http://ndjson.org/) JSON. The following CLI flags can be\nused to control how input is parsed:\n\n```\nUsage: csv-parser [filename?] [options]\n\n  --escape,-e         Set the escape character (defaults to quote value)\n  --headers,-h        Explicitly specify csv headers as a comma separated list\n  --help              Show this help\n  --output,-o         Set output file. Defaults to stdout\n  --quote,-q          Set the quote character ('\"' by default)\n  --remove            Remove columns from output by header name\n  --separator,-s      Set the separator character (\",\" by default)\n  --skipComments,-c   Skip CSV comments that begin with '#'. Set a value to change the comment character.\n  --skipLines,-l      Set the number of lines to skip to before parsing headers\n  --strict            Require column length match headers length\n  --version,-v        Print out the installed version\n```\n\nFor example; to parse a TSV file:\n\n```\ncat data.tsv | csv-parser -s $'\\t'\n```\n\n## Encoding\n\nUsers may encounter issues with the encoding of a CSV file. Transcoding the\nsource stream can be done neatly with a modules such as:\n- [`iconv-lite`](https://www.npmjs.com/package/iconv-lite)\n- [`iconv`](https://www.npmjs.com/package/iconv)\n\nOr native [`iconv`](http://man7.org/linux/man-pages/man1/iconv.1.html) if part\nof a pipeline.\n\n## Byte Order Marks\n\nSome CSV files may be generated with, or contain a leading [Byte Order Mark](https://en.wikipedia.org/wiki/Byte_order_mark#UTF-8). This may cause issues parsing headers and/or data from your file. From Wikipedia:\n\n>The Unicode Standard permits the BOM in UTF-8, but does not require nor recommend its use. Byte order has no meaning in UTF-8.\n\nTo use this module with a file containing a BOM, please use a module like [strip-bom-stream](https://github.com/sindresorhus/strip-bom-stream) in your pipeline:\n\n```js\nconst fs = require('fs');\n\nconst csv = require('csv-parser');\nconst stripBom = require('strip-bom-stream');\n\nfs.createReadStream('data.csv')\n  .pipe(stripBomStream())\n  .pipe(csv())\n  ...\n```\n\nWhen using the CLI, the BOM can be removed by first running:\n\n```console\n$ sed $'s/\\xEF\\xBB\\xBF//g' data.csv\n```\n\n## Meta\n\n[CONTRIBUTING](./.github/CONTRIBUTING)\n\n[LICENSE (MIT)](./LICENSE)\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/mafintosh/csv-parser.git"
  },
  "scripts": {
    "bench": "bin/bench",
    "commitlint": "commitlint",
    "commitmsg": "commitlint -e $GIT_PARAMS",
    "lint": "eslint .",
    "lint-staged": "lint-staged",
    "test": "ava"
  },
  "version": "2.2.0"
}
